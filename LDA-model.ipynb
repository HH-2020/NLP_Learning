{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyknp import Juman\n",
    "from pandas_ods_reader import read_ods\n",
    "import re\n",
    "#reference: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "#         : https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "\n",
    "#read the 5 point data\n",
    "df5 = read_ods(file_or_path =\"dt.ods\", sheet = 2)\n",
    "comments_data5 = df5.iloc[0:,8]\n",
    "comments_data5.index = range(len(comments_data5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comments_data5)\n",
    "###\n",
    "#for i in comments_data5:\n",
    "#    i = i.split(\"。\")\n",
    "#    print(i)\n",
    "#    text=re.split(r'( ^`^b|！|？)\\s*',i,maxsplit=10)\n",
    "#    for m in text:\n",
    "#       final = re.split(r'。',m)\n",
    "#       for j in final:\n",
    "#           if len(j) >=2:\n",
    "#               df.append(j)\n",
    "#df = pd.Series(df)\n",
    "#comments_data5=df\n",
    "#print(len(comments_data5))\n",
    "#print(comments_data5[0:5])\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wakeru\n",
    "juman = Juman()\n",
    "def wakeru(x):\n",
    "#    print(x)\n",
    "    result = juman.analysis(x)\n",
    "#for m in mrphs:\n",
    "#    print(m.midasi, m.genkei, m.hinsi, m.bunrui, m.katuyou2)\n",
    "    a = []\n",
    "    for mrph in result.mrph_list():\n",
    "        if mrph.hinsi ==\"名詞\" or mrph.hinsi == \"形容詞\" or mrph.hinsi ==\"動詞\":\n",
    "            if mrph.genkei != None:\n",
    "                a.append(mrph.genkei)\n",
    "    return a\n",
    "comment_pos = comments_data5.apply(wakeru)\n",
    "#print(comment[0:5])\n",
    "# read stopwords\n",
    "stopword_path = \"Japanese.txt\"\n",
    "st = pd.read_csv(stopword_path, encoding = 'utf-8', header = None)\n",
    "#print(comment[48]) #test\n",
    "stop_w = st.values.tolist()\n",
    "#print(type(stop_w)) test\n",
    "stw = list()\n",
    "for i in stop_w:\n",
    "    for j in i:\n",
    "        stw.append(j)\n",
    "#print(stw) #test\n",
    "#print('あそこ' not in stw) test\n",
    "a=0\n",
    "# delete stopwords\n",
    "for m in range(0,20):\n",
    "    for i in comment_pos:\n",
    "        for j in i:\n",
    "            if j not in stw:\n",
    "                j=j\n",
    "            else:\n",
    "                i.remove(j)\n",
    "len(comment_pos)\n",
    "\n",
    "print(comment_pos[0:3])\n",
    "print(\"--------------------------------\")\n",
    "print(comments_data5[0:3])\n",
    "\n",
    "print(comment_pos[0])\n",
    "print(comments_data5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models\n",
    "\n",
    "#Build LDA model\n",
    "#reference: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "#         : https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "#Build dictionary\n",
    "pos_dict = corpora.Dictionary(comment_pos)\n",
    "pos_corpus = [pos_dict.doc2bow(i) for i in comment_pos]\n",
    "#bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus=pos_corpus, texts=comments_data5\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    log_per=[]\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model =  models.LdaModel(corpus = corpus,\n",
    "                                  num_topics = num_topics,\n",
    "                                  id2word = pos_dict,\n",
    "                                  random_state = 30,\n",
    "                                  chunksize = 300,\n",
    "                                  passes = 10,\n",
    "                                  alpha = 'auto',\n",
    "                                  per_word_topics = True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=comment_pos, dictionary=pos_dict, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        log_per.append(model.log_perplexity(pos_corpus))\n",
    "    return model_list, coherence_values, log_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values, log_per = compute_coherence_values(dictionary=pos_dict, corpus=pos_corpus, texts=comments_data5, start=2, limit=30, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "# the bigger the better\n",
    "import matplotlib.pyplot as plt\n",
    "limit=30; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.savefig(\"Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the smaller the better\n",
    "limit=30; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "per = []\n",
    "for i in log_per:\n",
    "    per.append(np.exp2(-i))\n",
    "plt.plot(x, per)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.legend((\"Perplexity\"), loc='best')\n",
    "plt.savefig(\"Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, per):\n",
    "    print(\"Num Topics =\", m, \" has Perplexity Value of\", round(cv, 4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a topic nums\n",
    "num_topics_pos=3\n",
    "tfidf = models.TfidfModel(pos_corpus)\n",
    "corpus_tfidf = tfidf[pos_corpus]\n",
    "\n",
    "#pos_corpus = corpus_tfidf\n",
    "#lda\n",
    "pos_lda = models.LdaModel(corpus = pos_corpus,\n",
    "                          num_topics = num_topics_pos,\n",
    "                          id2word = pos_dict,\n",
    "                          random_state = 30,\n",
    "                          chunksize = 300,\n",
    "                          passes = 10,\n",
    "                          alpha = 'auto',\n",
    "                          per_word_topics = True) \n",
    "for i in range(num_topics_pos):\n",
    "    print('pos_topic' + str(i))\n",
    "    print(pos_lda.print_topic(i))\n",
    "\n",
    "\n",
    "print('\\nPerplexity: ', pos_lda.log_perplexity(pos_corpus))\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=pos_lda, texts=comment_pos, dictionary=pos_dict, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import MDS, TSNE\n",
    "vis = pyLDAvis.gensim.prepare(pos_lda, pos_corpus, pos_dict)\n",
    "pyLDAvis.save_html(vis,\"lda.html\")\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=pos_lda, corpus=pos_corpus, texts=comments_data5):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "    # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=pos_lda, corpus=pos_corpus, texts=comments_data5)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(5)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "sent_topics_sorteddf_mallet.to_csv('good1.csv')\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = [ 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
